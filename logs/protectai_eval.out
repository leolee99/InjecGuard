04/20/2025 05:02:33 - INFO -   Effective parameters:
04/20/2025 05:02:33 - INFO -     <<< batch_size: 32
04/20/2025 05:02:33 - INFO -     <<< beta1: 0.9
04/20/2025 05:02:33 - INFO -     <<< beta2: 0.999
04/20/2025 05:02:33 - INFO -     <<< checkpoint_path: logs
04/20/2025 05:02:33 - INFO -     <<< dataset_root: datasets
04/20/2025 05:02:33 - INFO -     <<< display: 10
04/20/2025 05:02:33 - INFO -     <<< epochs: 3
04/20/2025 05:02:33 - INFO -     <<< eps: 1e-08
04/20/2025 05:02:33 - INFO -     <<< eval_batch_size: 32
04/20/2025 05:02:33 - INFO -     <<< logs: logs
04/20/2025 05:02:33 - INFO -     <<< lr: 2e-05
04/20/2025 05:02:33 - INFO -     <<< max_length: 512
04/20/2025 05:02:33 - INFO -     <<< name: 0
04/20/2025 05:02:33 - INFO -     <<< resume: protectai
04/20/2025 05:02:33 - INFO -     <<< save_step: 200
04/20/2025 05:02:33 - INFO -     <<< save_thres: 0.8
04/20/2025 05:02:33 - INFO -     <<< seed: 42
04/20/2025 05:02:33 - INFO -     <<< train_set: datasets/train.json
04/20/2025 05:02:33 - INFO -     <<< valid_set: datasets/valid.json
04/20/2025 05:02:33 - INFO -     <<< warmup: 100
/home/hao/miniconda3/envs/injecguard/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/home/hao/miniconda3/envs/injecguard/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Using device: cuda
chat set accuracy: 0.9981684981684982
documents set accuracy: 0.988950276243094
hard_negatives set accuracy: 0.9295352323838081
public_prompt_injection set accuracy: 0.9333333333333333
internal_prompt_injection set accuracy: 0.865546218487395
jailbreak set accuracy: 0.8571428571428572
benign accuracy: 0.9722180022651333
injection accuracy: 0.8853408029878619
overall accuracy: 0.9287794026264976
wildguard set accuracy: 0.7518022657054583
BIPIA_text set accuracy: 0.17333333333333334
BIPIA_code set accuracy: 0.0
BIPIA overall accuracy: 0.08666666666666667
NotInject_one set accuracy: 0.7610619469026548
NotInject_two set accuracy: 0.47787610619469023
NotInject_three set accuracy: 0.4601769911504425
NotInject overall accuracy: 0.5663716814159292
================================ The Results ================================
Over-defense ACC: 0.5663716814159292
Benign ACC: 0.8620101339852958
Injection ACC: 0.4860037348272643
Overall ACC: 0.6381285167428298
